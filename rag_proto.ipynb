{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c665df-99b1-420a-806e-f9a363ce97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is an agent based implementation of question asnwer RAG\n",
    "based on user uploaded PDF files\n",
    "'''\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display\n",
    "from ipywidgets import FileUpload, Label\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.graph import END, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5929f96-f8af-4e31-98ec-df899a3eba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the needed parameters for OpenAI APIs\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('AZURE_OPENAI_API_KEY')\n",
    "api_endpoint = os.environ.get('AZURE_OPENAI_ENDPOINT')\n",
    "deployment_textgen = os.environ.get('AZURE_DEPLOYMENT_TEXTGEN')\n",
    "deployment_embed = os.environ.get('AZURE_DEPLOYMENT_EMBED')\n",
    "api_version = os.environ.get('AZURE_OPENAI_VERSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ec6e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining llm and embeddings models\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=api_endpoint,\n",
    "    openai_api_type='azure',\n",
    "    deployment=deployment_embed,\n",
    "    openai_api_key=api_key,\n",
    "    chunk_size=1\n",
    ")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=api_endpoint,\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_version=api_version,\n",
    "    deployment_name=deployment_textgen,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18d8e8b-1499-4c5d-8877-ee2fb6b555dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validator class for document relevance score\n",
    "\n",
    "class DocumentRelevaceScore(BaseModel):\n",
    "    ''' Simple class to format LLM generate JSON '''\n",
    "\n",
    "    score: str = Field(description=\"Relevance score. Values are yes or no\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=DocumentRelevaceScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e225197-3605-49e2-b573-4a3be2a448b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from persistent DB\n"
     ]
    }
   ],
   "source": [
    "def persistent_retriver(embed=embeddings, search_type=\"similarity\", k=3):\n",
    "    '''\n",
    "    Create a Chroma instance and a document retriver. If there is a\n",
    "    db directory exising then read from it. If not, initialize a new db\n",
    "\n",
    "    Args:\n",
    "        embeddings (LangchainEmbeddings): Embedding function to use for\n",
    "                                          retriever\n",
    "    '''\n",
    "\n",
    "    persist_directory = './chroma/'\n",
    "    if os.path.isdir(persist_directory):\n",
    "        print(\"Reading from persistent DB\")\n",
    "        vectordb = Chroma(\n",
    "            persist_directory=persist_directory,\n",
    "            embedding_function=embed\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating persistent DB\")\n",
    "        vectordb = Chroma(\n",
    "            embedding_function=embed,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "    return vectordb, vectordb.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs={\"k\": k}\n",
    "    )\n",
    "\n",
    "db, retriever = persistent_retriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25a63be-202c-493c-bea5-8c9d69ba65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index a file\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, chunk_overlap=50\n",
    ")\n",
    "\n",
    "def index_document(file, splitter=text_splitter, vectordb=db):\n",
    "    \"\"\"\n",
    "    Adds a document to vector database\n",
    "\n",
    "    Args:\n",
    "        file(str): path to file\n",
    "        splitter(langchain_text_splitters.base.TextSplitter): text splitter\n",
    "        vectordb(langchain_community.vectorstores.chroma.Chroma): DB instance\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        doc = PyPDFLoader(file).load()\n",
    "        chunks = splitter.split_documents(doc)\n",
    "        vectordb.add_documents(chunks)\n",
    "    except Exception as e:\n",
    "        print(f'{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c41ddb4-ad48-447a-bc45-a7364d6153f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da60dfa7c644295958eff4ded9c6c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f40f00dd0644a10a49ec99ac83ea284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Empty')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Upload widget allowing upload pdf files\n",
    "# The widget triggers file indexing upon upload\n",
    "\n",
    "# create 'docs' directory if it doesn't exist\n",
    "if not os.path.exists('docs'):\n",
    "    os.makedirs('docs')\n",
    "\n",
    "# create a FileUpload widget\n",
    "upload = FileUpload(accept='.pdf', multiple=False)\n",
    "label = Label(\"Empty\")\n",
    "\n",
    "def on_upload_change(change):\n",
    "    '''\n",
    "    Overwriting FileUpload listener to trigger file saving\n",
    "    and indexing actions\n",
    "    '''\n",
    "\n",
    "    file_name = change['new'][0]['name']\n",
    "    file_content = change['new'][0]['content']\n",
    "    label.value = file_name\n",
    "\n",
    "    # save file to documents directory\n",
    "    try:\n",
    "        with open(f'docs/{file_name}', 'wb') as f:\n",
    "            f.write(file_content)\n",
    "    except Exception as e:\n",
    "        print(f'File writing error: {e}')\n",
    "\n",
    "    # index the file and add to embedding store\n",
    "    index_document(f'docs/{file_name}')\n",
    "\n",
    "upload.observe(on_upload_change, names='value')\n",
    "display(upload, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6239b3a4-4aaa-4c1f-86e0-221b27e2e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Grader - checks if there are relevant documents\n",
    "# among the retived ones\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved\n",
    "    document to a user question. If the document contains keywords\n",
    "    related to the user question, grade it as relevant. It does not\n",
    "    need to be a stringent test. The goal is to filter out erroneous\n",
    "    retrievals. Give a binary score 'yes' or 'no' score to indicate\n",
    "    whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and\n",
    "    no premable or explaination.\\n\n",
    "    {format_instructions}\\n\\n\n",
    "    Here is the retrieved document: \\n {document} \\n\n",
    "    Here is the user question: {question} \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a722491-80da-420a-acff-2dd6e30e77ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question.\n",
    "    If you don't know the answer, just say that you don't know. Use only\n",
    "    information that is available in context.\n",
    "    Keep the answer concise\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    ''' During streaming concatenate everything to a string '''\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bceae111-5cfd-4246-98eb-f16e17f032f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functionalities of the graph and its nodes\n",
    "\n",
    "# State\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM answer generation\n",
    "        enough_context: whether there is context for generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    enough_context: str\n",
    "    documents: List[str]\n",
    "\n",
    "# Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents,\n",
    "        that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation,\n",
    "        that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to\n",
    "    the question. If any document is not relevant, we will set a\n",
    "    flag to indicate absense of the relevant context for generation\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated\n",
    "        enough_context state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    enough_context = \"yes\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.score\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to stop and not\n",
    "            # generate the answer\n",
    "            enough_context = \"no\"\n",
    "            continue\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"question\": question,\n",
    "        \"enough_context\": enough_context\n",
    "    }\n",
    "\n",
    "\n",
    "# Conditional edge\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or stop as there is not\n",
    "    context for the generation\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    enough_context = state[\"enough_context\"]\n",
    "\n",
    "    if enough_context == \"no\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        print(\n",
    "            \"---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO QUESTION, STOP---\"\n",
    "        )\n",
    "        return \"stop\"\n",
    "\n",
    "    # We have relevant documents, so generate answer\n",
    "    print(\"---DECISION: GENERATE---\")\n",
    "    return \"generate\"\n",
    "\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37bc400b-0432-40ae-8c6b-358171542d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph connecting nodes with edges\n",
    "\n",
    "workflow.set_entry_point(\n",
    "    \"retrieve\"\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"generate\": \"generate\",\n",
    "        \"stop\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"generate\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ad07ef-4e67-4544-9187-89bb558e417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3866a1a9-3f63-4558-ad07-b5126225634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_streaming(question):\n",
    "    '''\n",
    "    Helper funtion invoking the RAG graph\n",
    "\n",
    "    Args:\n",
    "        question(str): User input / question\n",
    "    '''\n",
    "\n",
    "    inputs = {\"question\": question}\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            pprint(f\"Finished running: {key}:\")\n",
    "\n",
    "    try:\n",
    "        value\n",
    "    except NameError:\n",
    "        print(\"Something went wrong generating answer\")\n",
    "\n",
    "    else:\n",
    "        if \"generation\" in value:\n",
    "            pprint(value[\"generation\"])\n",
    "        else:\n",
    "            print(\"There was not enough context for an answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f053810-701f-4a98-a5bb-39ab2de40ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "'Finished running: generate:'\n",
      "('Retrieval-Augmented Generative Agent (RAG) is an agent that performs '\n",
      " 'information retrieval across full-text scientific articles and uses RAG '\n",
      " 'models to provide answers to questions over the scientific literature.')\n"
     ]
    }
   ],
   "source": [
    "# Test 1 -\n",
    "\n",
    "qa_streaming(\"What is Retrieval-Augmented Generative Agent?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df20f7e5-b141-4927-ad75-3ebf321c06f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE---\n",
      "'Finished running: generate:'\n",
      "('Choose a mating partner for a dog by ensuring that both the sire and dam are '\n",
      " 'AKC registered. Select a dog whose bloodlines will strengthen the weaknesses '\n",
      " 'and emphasize the good qualities of the bitch. Consider the factors that '\n",
      " \"contribute to the dogs' traits and appearances. Study the pedigrees of the \"\n",
      " 'mating pair and be knowledgeable about the genetic problems that affect the '\n",
      " 'breed.')\n"
     ]
    }
   ],
   "source": [
    "# Test 2\n",
    "\n",
    "qa_streaming(\"How to choose a mating partner for a dog?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20dfbf81-0cea-4853-881c-a40324c91411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO QUESTION, STOP---\n",
      "'Finished running: grade_documents:'\n",
      "There was not enough context for an answer\n"
     ]
    }
   ],
   "source": [
    "# Test 3\n",
    "\n",
    "qa_streaming(\"What are the main difference between Volvo and Kia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b2ac51-1b68-43b6-9b37-2c8d48c2663c",
   "metadata": {},
   "source": [
    "### Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b040d711-fd77-4de9-8c9e-6dfb8dc153dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'safas'\n",
      "EOF marker not found\n",
      "EOF marker not found\n",
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream has ended unexpectedly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 7.900s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestIndexDocument(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.file_path = 'test_docs/guide_to_breeding_your_dog.pdf'\n",
    "        self.splitter = text_splitter\n",
    "        self.vectordb = db\n",
    "\n",
    "    def test_index_document_valid_file(self):\n",
    "        index_document(self.file_path, self.splitter, self.vectordb)\n",
    "        self.assertTrue(True)  # Add an assertion to verify the expected behavior\n",
    "\n",
    "    def test_index_document_invalid_file_format(self):\n",
    "        try:\n",
    "            index_document('test_docs/bad_pdf.pdf', self.splitter, self.vectordb)\n",
    "        except Exception as e:\n",
    "            self.assertEqual(str(e), 'Wrong PDF format: Invalid file format')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
